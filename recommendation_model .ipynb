{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f865c0ac-0daf-4f12-807c-bd636e4240d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 14:19:35.883136: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:35.899632: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 16.451518ms.\n",
      "2024-05-12 14:19:36.338702: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:36.356969: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 18.223871ms.\n",
      "2024-05-12 14:19:36.775973: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:36.803925: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 27.887433ms.\n",
      "2024-05-12 14:19:37.231110: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:37.258916: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 27.744171ms.\n",
      "2024-05-12 14:19:37.703033: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:37.726106: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 23.004729ms.\n",
      "2024-05-12 14:19:38.155107: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:38.187202: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 32.039929ms.\n",
      "2024-05-12 14:19:38.620422: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:38.650595: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 30.12183ms.\n",
      "2024-05-12 14:19:39.086725: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:39.117543: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 30.768613ms.\n",
      "2024-05-12 14:19:39.562759: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:39.595361: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 32.545247ms.\n",
      "2024-05-12 14:19:40.030692: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:40.057255: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 26.500974ms.\n",
      "2024-05-12 14:19:40.488940: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:40.521399: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 32.412798ms.\n",
      "2024-05-12 14:19:40.962653: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:40.996101: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 33.391302ms.\n",
      "2024-05-12 14:19:41.430648: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:41.464283: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 33.590535ms.\n",
      "2024-05-12 14:19:41.904955: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:41.939139: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 34.114541ms.\n",
      "2024-05-12 14:19:42.363225: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:42.393060: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 29.783043ms.\n",
      "2024-05-12 14:19:42.844485: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:42.881676: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 37.144529ms.\n",
      "2024-05-12 14:19:43.312437: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:43.346263: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 33.77783ms.\n",
      "2024-05-12 14:19:43.773696: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:43.801087: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 27.346486ms.\n",
      "2024-05-12 14:19:44.238477: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:44.272112: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 33.589852ms.\n",
      "2024-05-12 14:19:44.700501: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:44.731360: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 30.798274ms.\n",
      "2024-05-12 14:19:45.171881: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:45.208868: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 36.916247ms.\n",
      "2024-05-12 14:19:45.636484: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:45.678029: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 41.49671ms.\n",
      "2024-05-12 14:19:46.107981: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:46.149025: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 40.985029ms.\n",
      "2024-05-12 14:19:46.593595: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:46.631612: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 37.966532ms.\n",
      "2024-05-12 14:19:47.066282: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:47.107227: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 40.884702ms.\n",
      "2024-05-12 14:19:47.538777: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:47.561341: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 22.51046ms.\n",
      "2024-05-12 14:19:47.993795: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1662\n",
      "2024-05-12 14:19:48.016248: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 22.394562ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Calculating silhouette score based on clustering...\n",
      "Silhouette Score: 0.2252306704302388\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_to_your_audio_file.mp3 /home/mutayyab/Documents/dataset/091/091125.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Top Recommendations Based on Rhythmic Similarity:\n",
      " \n",
      "Recommended Track: 091643.mp3 - Similarity Score: 0.369567334651947\n",
      "Recommended Track: 091992.mp3 - Similarity Score: 0.3840720057487488\n",
      "Recommended Track: 090003.mp3 - Similarity Score: 0.44844454526901245\n",
      "Recommended Track: 093442.mp3 - Similarity Score: 0.4810391068458557\n",
      "Recommended Track: 091635.mp3 - Similarity Score: 0.4901089072227478\n",
      "Recommended Track: 091773.mp3 - Similarity Score: 0.5003558993339539\n",
      "Recommended Track: 092273.mp3 - Similarity Score: 0.5029585659503937\n",
      "Recommended Track: 091828.mp3 - Similarity Score: 0.5037169456481934\n",
      "Recommended Track: 091268.mp3 - Similarity Score: 0.5423231720924377\n",
      "Recommended Track: 092201.mp3 - Similarity Score: 0.5428546667098999\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scann\n",
    "import librosa\n",
    "import os\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load precomputed features from the CSV file\n",
    "data_path = \"/home/mutayyab/Documents/dataset/preprocessed_audios.csv\"  # Update this path if needed\n",
    "features_df = pd.read_csv(data_path)\n",
    "\n",
    "# Assume features are all columns after 'filename'\n",
    "feature_columns = features_df.columns[1:448]\n",
    "features = features_df[feature_columns].values\n",
    "normalized_features = tf.math.l2_normalize(features, axis=1)\n",
    "\n",
    "# Define ScannSearcherWrapper class for GridSearchCV\n",
    "class ScannSearcherWrapper(BaseEstimator):\n",
    "    def __init__(self, features, num_neighbors=10, num_leaves=10, num_leaves_to_search=5):\n",
    "        self.features = features\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.num_leaves = num_leaves\n",
    "        self.num_leaves_to_search = num_leaves_to_search\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.searcher = self.create_scann_index(self.features, self.num_neighbors, self.num_leaves, self.num_leaves_to_search)\n",
    "        return self\n",
    "    \n",
    "    def create_scann_index(self, features, num_neighbors, num_leaves, num_leaves_to_search):\n",
    "        searcher = scann.scann_ops_pybind.builder(\n",
    "            features, num_neighbors, \"dot_product\"\n",
    "        ).tree(\n",
    "            num_leaves=num_leaves, num_leaves_to_search=num_leaves_to_search\n",
    "        ).score_ah(\n",
    "            2, anisotropic_quantization_threshold=0.2\n",
    "        ).reorder(100).build()\n",
    "        return searcher\n",
    "    \n",
    "    def search(self, X, n_neighbours=5):\n",
    "        return self.get_nearest_neighbours(X, self.searcher, n_neighbours)\n",
    "    \n",
    "    def get_nearest_neighbours(self, feature_vector, searcher, n_neighbours=5):\n",
    "        # Ensure the vector is one-dimensional and a numpy array\n",
    "        if feature_vector.ndim != 1 or not isinstance(feature_vector, np.ndarray):\n",
    "            feature_vector = np.array(feature_vector).flatten()\n",
    "\n",
    "        neighbors, distances = searcher.search(feature_vector, final_num_neighbors=n_neighbours)\n",
    "        return neighbors, distances\n",
    "\n",
    "# Define a grid of values for num_leaves\n",
    "param_grid = {'num_leaves': [10, 20, 30, 40, 50]}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(estimator=ScannSearcherWrapper(features=normalized_features), param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(normalized_features.numpy())\n",
    "\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_num_leaves = grid_search.best_params_['num_leaves']\n",
    "\n",
    "# Update the create_scann_index function with the best num_leaves\n",
    "def create_scann_index(features, num_neighbors=10, num_leaves=best_num_leaves, num_leaves_to_search=5):\n",
    "    searcher = scann.scann_ops_pybind.builder(\n",
    "        features, num_neighbors, \"dot_product\"\n",
    "    ).tree(\n",
    "        num_leaves=num_leaves, num_leaves_to_search=num_leaves_to_search\n",
    "    ).score_ah(\n",
    "        2, anisotropic_quantization_threshold=0.2\n",
    "    ).reorder(100).build()\n",
    "    return searcher\n",
    "\n",
    "# Create the ScaNN searcher with the best num_leaves\n",
    "scann_searcher = create_scann_index(normalized_features)\n",
    "\n",
    "def load_and_query(audio_path, features_df, searcher, n_neighbours=5):\n",
    "    if not os.path.exists(audio_path):\n",
    "        print(\"File not found. Please check the path and try again.\")\n",
    "        return\n",
    "\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc_mean = mfcc.mean(axis=1)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=50)\n",
    "    spectrogram = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    spectrogram_mean = spectrogram.mean(axis=1)\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    tempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr)\n",
    "    tempogram_mean = tempogram.mean(axis=1)\n",
    "\n",
    "    feature_vector = np.hstack([mfcc_mean, spectrogram_mean, tempogram_mean])\n",
    "    if np.linalg.norm(feature_vector) > 0:\n",
    "        feature_vector_normalized = feature_vector / np.linalg.norm(feature_vector)\n",
    "    else:\n",
    "        feature_vector_normalized = feature_vector\n",
    "\n",
    "    nearest_neighbours, distances = get_nearest_neighbours(feature_vector_normalized, searcher, n_neighbours)\n",
    "    print(\" \")\n",
    "    print(\"Top Recommendations Based on Rhythmic Similarity:\")\n",
    "    print(\" \")\n",
    "    for neighbour, distance in zip(nearest_neighbours, distances):\n",
    "        print(f\"Recommended Track: {features_df.iloc[neighbour]['filename']} - Similarity Score: {1 - distance}\")\n",
    "\n",
    "def get_nearest_neighbours(feature_vector, searcher, n_neighbours=5):\n",
    "    # Ensure the vector is one-dimensional and a numpy array\n",
    "    if feature_vector.ndim != 1 or not isinstance(feature_vector, np.ndarray):\n",
    "        feature_vector = np.array(feature_vector).flatten()\n",
    "\n",
    "    neighbors, distances = searcher.search(feature_vector, final_num_neighbors=n_neighbours)\n",
    "    return neighbors, distances\n",
    "\n",
    "def classify_rhythmic_features(df):\n",
    "    print(\" \")\n",
    "    print(\"Calculating silhouette score based on clustering...\")\n",
    "    \n",
    "    # Extract the feature columns for silhouette score calculation\n",
    "    feature_columns = [col for col in df.columns if col.startswith('tempo')]\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(df[feature_columns], df['cluster'])\n",
    "    print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Perform clustering and calculate silhouette score\n",
    "features_df = classify_rhythmic_features(features_df)\n",
    "print(\" \")\n",
    "audio_path = input(\"path_to_your_audio_file.mp3\") # Update this path\n",
    "load_and_query(audio_path, features_df, scann_searcher, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d157eaa-c70c-4210-80ed-28f5bc2d15a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
